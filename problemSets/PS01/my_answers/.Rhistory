#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Set seed, then creating data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
data
data <- rcauchy(1000, location = 0, scale = 1)
data
# Set seed, then creating data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Set seed, then creating data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
kolsmir <- function(data, D){
x <- mean(data)
k <- nrow(data)
D <- D
func <- exp(1)^ -((((2*k - 1) ^2 )*( pi^2 )) / D)
( sqrt( 2*pi )) / D
test <- lapply(x = data, FUN = func)
return(test)
}
# Implement test
result <- kolsmir(data = data, D = D)
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# remove objects
rm(list=ls())
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
lm_result <- lm(y ~ x, data = data)
lm_result
attributes(lm_result)
lm_result$coefficients
summarise(lm_result$coefficients)
Summarise(lm_result$coefficients)
summary(lm_result$coefficients)
lm_result$coefficients
?coef
?optim
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Use optim() function to estimate OLS regression
# Set starting values for the parameters as c(1, 1) to be optimised over
bfgs_model <- optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares (ie differences between the model and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Extract coefficients for comparison
bfgs_coefficients <- coef(bfgs_model)
# Generate OLS regression using lm() for comparison
lm_compare <- lm(y ~ x, data = data)
# Extract coefficients for comparison
lm_coefficients <- lm_compare$coefficients
bfgs_coefficients
coef(bfgs_model)
# Use optim() function to estimate OLS regression
# Set starting values for the parameters as c(1, 1) to be optimised over
bfgs_model <- optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares (ie differences between the model and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Use optim() function to estimate OLS regression
# Set starting values for the parameters as c(1, 1) to be optimised over
bfgs_func <- optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares (ie differences between the model and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Extract coefficients for comparison
bfgs_coefficients <- coef(bfgs_model)
# Generate OLS regression using lm() for comparison
lm_compare <- lm(y ~ x, data = data)
# Extract coefficients for comparison
lm_coefficients <- lm_compare$coefficients
lm_coefficients
bfgs_coefficients
optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares (ie differences between the model and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Extract coefficients for comparison
bfgs_coefficients <- bfgs_model$par
lm_coefficients
bfgs_coefficients
identical(lm_coefficients, bfgs_coefficients)
all.equal(lm_coefficients, bfgs_coefficients)
lm_coefficients
df_check <- data.frame(lm_coefficients, bfgs_coefficients)
df_check
?data.frame
df_check <- table(lm_coefficients, bfgs_coefficients)
df_check
df_check <- tibble(lm_coefficients, bfgs_coefficients)
df_check <- tle(lm_coefficients, bfgs_coefficients)
df_check <- tbl(lm_coefficients, bfgs_coefficients)
df_check <- data.frame(lm_coefficients, bfgs_coefficients, row.names = NULL)
df_check
all.equal(df_check[1,])
all.equal(df_check[1])
all.equal(df_check[,1])
df_check[,1]
all.equal(df_check[1,:])
all.equal(df_check[1,])
all.equal(df_check[1, ])
identical(df_check[1,])
identical(df_check[1, 1], df_check[1, 2])
identical(df_check[1, 1], df_check[2, 1])
all.equal(df_check[1, 1], df_check[2, 1])
all.equal(df_check[1, 1], df_check[1, 2])
df_check[1, 2]
df_check[1, 1]
all.equal(df_check[, 1], df_check[, 2])
df_check[, 1]
all.equal(df_check[1, 1], df_check[1, 2])
all.equal(df_check[, 1], df_check[, 2])
all.equal(lm_coefficients, bfgs_coefficients)
?all.equal
all.equal(lm_coefficients, bfgs_coefficients, check.attributes = FALSE)
all.equal(lm_coefficients, bfgs_coefficients, check.attributes = FALSE)
?identical
identical(lm_coefficients, bfgs_coefficients)
# Compare outputs using all.equal() function in base R, ignoring differences in attributes such as column names.
model_check <- all.equal(lm_coefficients, bfgs_coefficients, check.attributes = FALSE)
model_check
# remove objects
rm(list=ls())
# Set seed for reproducibility, then generate data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
print(model_check) # Value is TRUE, confirming my specified output is correct
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Use optim() function to estimate OLS regression
# Set starting values for the parameters as c(1, 1) to be optimised over
bfgs_func <- optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares
# (ie differences between the model's predicted values and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Extract coefficients for comparison
bfgs_coefficients <- bfgs_model$par
# Set seed
set.seed(123)
# Generate dataframe with variable x
data <- data.frame(x = runif(200, 1, 10))
# Add y variable
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Use optim() function to estimate OLS regression
# Set starting values for the parameters as c(1, 1) to be optimised over
bfgs_model <- optim(par = c(1, 1),
# Set BFGS algorithm for the Newton-Raphson optimisation method
method = "BFGS",
# Lambda function calculates the residual sum of squares
# (ie differences between the model's predicted values and observed values)
fn = function(coef) {
sum((data$y - (coef[1] + coef[2]*data$x)) ^2) })
# Extract coefficients for comparison
bfgs_coefficients <- bfgs_model$par
# Generate OLS regression using lm() for comparison
lm_compare <- lm(y ~ x, data = data)
# Extract coefficients for comparison
lm_coefficients <- lm_compare$coefficients
# Compare outputs using all.equal() function in base R, ignoring differences in attributes such as column names.
model_check <- all.equal(lm_coefficients, bfgs_coefficients, check.attributes = FALSE)
print(model_check) # Value is TRUE, confirming my specified output is correct
# remove objects
rm(list=ls())
# Set seed for reproducibility, then generate data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
data
?ks.test
# Set seed for reproducibility, then generate data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Create empirical distribution for reference
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Test statistic for reference
D <- max(abs(empiricalCDF - pnorm(data)))
D
?theoretical_cdf
?pcauchy
# Conduct KS test
kolsmir_result <- kolsmir_statistic > kolsmir_critical
# Create Kolmogorov-Smirnov test function
kolsmir <- function(data){
# Sort input data
data_sorted <- sort(data)
# Calculate empirical CDF values
ecdf_values <- (1: length(data_sorted))/ length(data_sorted)
# Calculate theoretical CDF values
tcdf_values <- pcauchy(data_sorted, location = 0, scale = 1)
# Calculate maximum absolute difference between empirical and theoretical
kolsmir_statistic <- max(abs(ecdf_values - tcdf_values))
# Calculate critical value for Logo
n <- length(data)
kolsmir_critical <- sqrt(-0.5 * log(0.05 / 2) / sqrt(n))
# Conduct KS test
kolsmir_result <- kolsmir_statistic > kolsmir_critical
return(list("KS Test Statistic" = kolsmir_statistic, "KS Critical Value" = kolsmir_critical, "KS Test Result" = kolsmir_result))
}
# Implement Kolmogorov-Smirnov test function with generated data
result <- kolsmir(data = data)
result
return(list("KS_Test_Statistic" = kolsmir_statistic, "KS_Critical_Value" = kolsmir_critical, "KS_Test_Result" = kolsmir_result))
# Create Kolmogorov-Smirnov test function
kolsmir <- function(data){
# Sort input data
data_sorted <- sort(data)
# Calculate empirical CDF values
ecdf_values <- (1: length(data_sorted))/ length(data_sorted)
# Calculate theoretical CDF values
tcdf_values <- pcauchy(data_sorted, location = 0, scale = 1)
# Calculate maximum absolute difference between empirical and theoretical
kolsmir_statistic <- max(abs(ecdf_values - tcdf_values))
# Calculate critical value for Logo
n <- length(data)
kolsmir_critical <- sqrt(-0.5 * log(0.05 / 2) / sqrt(n))
# Conduct KS test
kolsmir_result <- kolsmir_statistic > kolsmir_critical
return(list("KS_Test_Statistic" = kolsmir_statistic, "KS_Critical_Value" = kolsmir_critical, "KS_Test_Result" = kolsmir_result))
}
# Implement Kolmogorov-Smirnov test function with generated data
result <- kolsmir(data = data)
result
# Create empirical distribution for reference
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Test statistic for reference
D <- max(abs(empiricalCDF - pnorm(data)))
# Implement Kolmogorov-Smirnov test function with generated data
data_result <- kolsmir(data = data)
data_result$KS_Test_Statistic
D
# Set seed for reproducibility, then generate data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Create empirical distribution for reference
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Test statistic for reference
D <- max(abs(empiricalCDF - pnorm(data)))
# Create Kolmogorov-Smirnov test function
kolsmir <- function(data){
# Sort input data
data_sorted <- sort(data)
# Calculate empirical CDF values
ecdf_values <- (1: length(data_sorted))/ length(data_sorted)
# Calculate theoretical CDF values
tcdf_values <- pcauchy(data_sorted, location = 0, scale = 1)
# Calculate maximum absolute difference between empirical and theoretical
kolsmir_statistic <- max(abs(ecdf_values - tcdf_values))
# Calculate critical value for Logo
n <- length(data)
kolsmir_critical <- sqrt(-0.5 * log(0.05 / 2) / sqrt(n))
# Conduct KS test
kolsmir_result <- kolsmir_statistic > kolsmir_critical
return(list("KS_Test_Statistic" = kolsmir_statistic, "KS_Critical_Value" = kolsmir_critical, "KS_Test_Result" = kolsmir_result))
}
# Implement Kolmogorov-Smirnov test function with generated data
data_result <- kolsmir(data = data)
D
data_result$KS_Test_Statistic
KS_compare <- data.frame("Custom Function" = data_result$KS_Test_Statistic, "Reference" = D)
KS_compare
ECDF
empiricalCDF
?abs
# Compare Reference and Custom Function Outputs
KS_compare <- data.frame("Custom Function" = data_result$KS_Test_Statistic, "Reference" = D, "Difference" = abs(data_result$KS_Test_Statistic - D))
KS_compare
(1: length(data_sorted))
# Set seed
set.seed(123)
# remove objects
rm(list=ls())
# Set seed for reproducibility, then generate data
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
# Create empirical distribution for reference
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Test statistic for reference
D <- max(abs(empiricalCDF - pnorm(data)))
# Create Kolmogorov-Smirnov test function
kolsmir <- function(data){
# Sort input data
data_sorted <- sort(data)
# Calculate empirical CDF values
ecdf_values <- (1: length(data_sorted)) / length(data_sorted)
# Calculate theoretical CDF values
tcdf_values <- pcauchy(data_sorted, location = 0, scale = 1)
# Calculate maximum absolute difference between empirical and theoretical
kolsmir_statistic <- max(abs(ecdf_values - tcdf_values))
# Calculate critical value for Logo
n <- length(data)
kolsmir_critical <- sqrt(-0.5 * log(0.05 / 2) / sqrt(n))
# Conduct KS test
kolsmir_result <- kolsmir_statistic > kolsmir_critical
return(list("KS_Test_Statistic" = kolsmir_statistic, "KS_Critical_Value" = kolsmir_critical, "KS_Test_Result" = kolsmir_result))
}
# Implement Kolmogorov-Smirnov test function with generated data
data_result <- kolsmir(data = data)
# Compare Reference and Custom Function Outputs
KS_compare <- data.frame("Custom Function" = data_result$KS_Test_Statistic, "Reference" = D, "Difference" = abs(data_result$KS_Test_Statistic - D))
KS_compare
print(KS_compare)
# Show how the Test Statistics compare between my custom function and the one generated
print(KS_compare)
# Create Kolmogorov-Smirnov test function
kolsmir <- function(data, p_value = 0.05){
# Sort input data
data_sorted <- sort(data)
# Calculate empirical CDF values
ecdf_values <- (1: length(data_sorted)) / length(data_sorted)
# Calculate theoretical CDF values
tcdf_values <- pcauchy(data_sorted, location = 0, scale = 1)
# Calculate maximum absolute difference between empirical and theoretical
kolsmir_statistic <- max(abs(ecdf_values - tcdf_values))
# Calculate p-value
kolsmir_p_value <- (sqrt(2 * pi)/kolsmir_statistic) * sum(exp( -((2 * nrow(data_sorted)) - 1)^2 * pi^2 / (8 * kolsmir_statistic^2)))
# Confirm
kolsmir_result <- kolsmir_p_value < p_value
return(list("KS_Test_Statistic" = kolsmir_statistic, "KS_P-Value" = kolsmir_p_value, "KS_Test_Result" = kolsmir_result))
}
# Implement Kolmogorov-Smirnov test function with generated data
data_result <- kolsmir(data = data)
# Compare Reference and Custom Function Outputs
KS_compare <- data.frame("Custom Function" = data_result$KS_Test_Statistic, "Reference" = D, "Difference" = abs(data_result$KS_Test_Statistic - D))
# Show how the Test Statistics compare between my custom function and the
print(KS_compare)
print(data_result)
# Show how the Test Statistics compare between my custom function and the
print(KS_compare)
